
{
    
    
    
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
            
                
            
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
    "pages": [{"date":"2023-02-10","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/%C6%B0%E1%BB%9Bc-m%C6%A1-c%E1%BB%A7a-m%E1%BA%B9/","summary":"Hello bạn, mình không biết bạn là ai, nhưng cảm ơn bạn vì đã ghé vào đây và nghe mình xàm xí. Bài viết này được viết lúc mình đang hơi tâm trạng, có thể làm ảnh hưởng đến bạn. Vậy nên bạn hãy cân nhắc tắt tivi hoặc chuyển kênh qua các kênh như #thơ hoặc các bài về tech nhé.","tags":["Tâm sự của NgocTD"],"text":"hello bạn, mình không biết bạn là ai, nhưng cảm ơn bạn vì đã ghé vào đây và nghe mình xàm xí. bài viết này được viết lúc mình đang hơi tâm trạng, có thể làm ảnh hưởng đến bạn. vậy nên bạn hãy cân nhắc tắt tivi hoặc chuyển kênh qua các kênh như #thơ hoặc các bài về tech nhé. cảm ơn nhiều nhiềuuu.\nà ừ, tuổi thơ của cậu thế nào nhỉ?\nmỗi khi mình nghe ai đó kể vể tuổi thơ thì mình sẽ luôn nhận ra là nó đẹp. mình cũng thế, mình lớn lên trong nghèo khó và nó cũng rất đẹp.\nà ừ, vài mảnh kí ức mà cậu còn nhớ?\nkhông biết cậu thế nào, chứ mình nhớ rất nhiều chuyện từ hồi mình bé xíu xíu và hầu hết nó đều liên quan đến mẹ \u0026hellip; còn nhớ cái hồi mình học lớp một, hồi đó nhà mình nghèo lắm. bố, mẹ mình làm đủ nghề đánh cá, trồng cà phê, thu mua đồng nát \u0026hellip; còn nhiều, nhiều lắm, mình cũng chả nhớ nổi nữa.\nmọi hôm thì mẹ sẽ chở mình đến trường, rồi sau đó sẽ đi thu mua đồng nát. nhưng hôm ấy, mẹ chở mình xong tiện thể sẽ đi làm luôn. đi đến gần trường, mẹ có hỏi mình rằng:\ncon có ngại với các bạn không? hồi đó mình cũng chả hiểu câu hỏi của mẹ đâu. mình trả lời là con không mẹ. nhưng mà lúc đó mẹ vẫn không đưa mình đến tận cổng, mẹ dừng ở xa xa một chút \u0026hellip;\nhồi đó, xung quanh nhà mình chả có ai cả, không làng, không xóm. bố mẹ thì đi làm suốt, chỉ có hai chị em chơi với nhau thôi. thân lắmmm. còn nhớ vào mỗi mùa khô, bố mẹ phải đi tưới cà phê ở rẫy phải vài ngày mới về. hai chị em ở nhà sợ lắm. nhà mình còn chả có đèn điện. tối đến hai chị em thắp nến lên, chắc hẳn ai cũng từng chỉ chỉ cái bóng khổng lồ xong chơi đùa với nó. còn mình và chị thì bảo nó là mẹ. mẹ dọa là tối thì không được mở cửa nhà, không thì trộm nó vào nhà đấy. thế mà chị mình đóng vai là mẹ, xong hai chị em cứ xưng hô mẹ, con với nhau. buồn cười lắm. à mình có viết một bài về cái vụ này, bạn có thể đọc ở đây nhé :3\nhồi cuối năm lớp 3, chị mình đòi về miền bắc (trước đó mình ở tây nguyên), bố mẹ mình cũng thấy trong đó điều kiện học hành không tốt, nên cho cả mình về. mình vui lắm. nhưng mình đâu có biết, là mình sẽ về sống với ông bà thôi, còn bố mẹ thì vẫn phải đi làm ở trong đó.\nmình nhớ có một lần mẹ về quê thăm con. trước tối hôm mẹ quay lại miền nam, mình bảo mẹ lúc nào mẹ dậy thì mẹ bảo con với nhé. mình còn cẩn thận lấy cái dây, buộc tay mình với mẹ lại, lúc đó mình sợ mẹ không gọi mình dậy. mình nhớ như in lúc 4h sáng hôm đấy, mình vẫn dậy được. lúc mẹ chuẩn bị đi, mình chạy đến ôm chân mẹ, khóc nức nở, không cho mẹ đi. mình cũng không nhớ mình nói gì với mẹ, chỉ nhớ mang máng là con không cho mẹ đi, mẹ ở nhà với con. mình khóc to, to lắm, to đến nỗi cả xóm sang khuyên mình cơ mà. bình thường mình mà khóc thì mẹ còn đánh thêm cho, nhưng hôm đấy dĩ nhiên là khác \u0026hellip;\nà ừ, mẹ khóc mẹ mình khóc nhiều lắm. bà ngoại có nói với mình là, trong cái nhà này không ai khổ bằng bà với mẹ (à mà mình có người bà tuyệt vời lắm, nếu có khúc mắc, chuyện buồn gì mà nói với bà là lúc sau sẽ được giải tỏa ngay. ước gì mình có thể nói chuyện giỏi như bà :3).\nhồi chị mình học năm 3, chị mình thấy không phù hợp nên đã bỏ học. đấy là lần đầu tiên mình thấy bố khóc. còn mẹ thì khóc cả ngày.\ncòn hôm nay mình cũng làm mẹ khóc. mẹ mình có ước mơ là mình học ths, mình đã nhận lời rồi, ấy thế mà hôm nay mình lại định bỏ. à mà không phải là mình không muốn học ths đâu, mà mình đang có một vài dự định khác, thôi thì cân tất vậy :3\nmình làm mẹ khóc nhiều lắm, nhưng mà thôi không dám kể :))\ncảm ơn bạn đã đọc đến tận lúc này, nếu được thì ib để mình mời cf nhé. cảm ơn nhiều nhiềuuu.\n","title":"Ước mơ của mẹ"},{"date":"2022-12-27","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/programming-language-channel-usecase/","summary":"This article will show many channel use cases\nAsynchronous and concurrency programing with Go channels is easy and enjoyable. The channel synchronization technique has wider range of uses and have more variables than the synchronization solutions used in some other languages, such as the actor model and the async/await pattern.","tags":["basic","golang"],"text":"this article will show many channel use cases\nasynchronous and concurrency programing with go channels is easy and enjoyable. the channel synchronization technique has wider range of uses and have more variables than the synchronization solutions used in some other languages, such as the actor model and the async/await pattern. use channels as futures/promises return receive-only channels as results\nfunc longtimerequest() \u0026lt;-chan struct{} { ch := make(chan struct{}) go func() { // simulate workload run 3s in using goroutine time.sleep(time.second * 3) ch \u0026lt;- struct{}{} }() // return immediately return ch } func main() { now := time.now() ch1 := longtimerequest() ch2 := longtimerequest() _, _ = \u0026lt;-ch1, \u0026lt;-ch2 // get result in future log.println(\u0026#34;exec in: \u0026#34;, time.since(now)) // ~ 3s } pass send-only channels as arguments\nfunc longtimerequest(ch chan\u0026lt;- struct{}) { // simulate workload run 3s in using goroutine ch \u0026lt;- struct{}{} log.println(\u0026#34;send value to channel\u0026#34;) } func main() { now := time.now() // buffer = 2 to avoid block to handle channle receive ch := make(chan struct{}, 2) go longtimerequest(ch) go longtimerequest(ch) \u0026lt;-ch \u0026lt;-ch log.println(\u0026#34;exec in: \u0026#34;, time.since(now)) } the first response win\nsometimes, a piece of data can be received from several sources to avoid high latencies. for a lot of factors, the response durations of these sources may vary much. to make the response duration as short as possible, we can send a request to every source in separated goroutine. only the first response use case will be used, other slower ones will be discarded.\nfunc source(c chan\u0026lt;- struct{}) { c \u0026lt;- struct{}{} } func main() { // c must be a buffered channel // if there are n sources, the capacity of the communication channel must be at least n-1 // to avoid the goroutines corresponding the discarded responses being blocked for ever c := make(chan struct{}, 5) for i := 0; i \u0026lt; cap(c); i++ { go source(c) } // first response win \u0026lt;-c } use channels for notifications notifications can be viewed as special requests/responses in which the responded values are not important. generally, we use the blank type struct{} as the element types of the notification channels.\n1-to-1 notification by sending a value to a channel\nif there are no values to be received from a channel, then the next receive operation on the channel will block until another goroutine sends a value to the channel. so we can send a value to a channel to notify another goroutine which is waiting to receive a value from the same channel.\nfunc blocking(c chan\u0026lt;- struct{}) { time.sleep(1 * time.second) // unlock by notifycation c \u0026lt;- struct{}{} } func main() { now := time.now() ch := make(chan struct{}) go blocking(ch) // blocking \u0026lt;-ch log.println(\u0026#34;since: \u0026#34;, time.since(now)) sig := make(chan os.signal) signal.notify(sig, os.interrupt) \u0026lt;-sig } 1-to-1 notification by receiving a value from a channel\nif the value buffer queue of a channel is full (the buffer queue of an unbuffered channel is always full), a send operation on the channel will block until another goroutine receives a value from the channel. so we can receive a value from a channel to notify another goroutine which is waiting to send a value to the same channel.\nfunc blocking(c \u0026lt;-chan struct{}) { time.sleep(1 * time.second) // unblock the second send in main goroutine \u0026lt;-c } func main() { now := time.now() ch := make(chan struct{}) go blocking(ch) // blocked here, wait for a notification ch \u0026lt;- struct{}{} log.println(\u0026#34;since: \u0026#34;, time.since(now)) sig := make(chan os.signal) signal.notify(sig, os.interrupt) \u0026lt;-sig } n-to-1 and 1-to-n notifications\nby making using of the feature that infinite values can be received from a closed channel, we can close a channel to broadcast notifications (1-to-n 1 main to n worker). use sync.waitgroup for n-to-1(n worker to 1 main) notification\nfunc worker(ready chan struct{}, wg *sync.waitgroup) { defer wg.done() // n-to-1 notification \u0026lt;-ready // block here and wait notification time.sleep(time.second) log.println(\u0026#34;run\u0026#34;) } func main() { ready := make(chan struct{}) wg := \u0026amp;sync.waitgroup{} wg.add(3) go worker(ready, wg) go worker(ready, wg) go worker(ready, wg) close(ready) wg.wait() } timer: scheduled notification\na custom one-time timer notification\nfunc after(duration time.duration) \u0026lt;-chan struct{} { ch := make(chan struct{}) go func() { time.sleep(duration) ch \u0026lt;- struct{}{} }() return ch } func main() { now := time.now() \u0026lt;-after(time.second) fmt.println(time.since(now)) } use channels as mutex locks there are two manners to use one-capacity buffered channels as mutex locks\nlock through a send, unlock through a receive func main() { cnt := 0 wg := sync.waitgroup{} lock := make(chan struct{}, 1) var increase = func() { defer wg.done() // lock through send lock \u0026lt;- struct{}{} cnt++ // unlock \u0026lt;-lock } for i := 0; i \u0026lt; 10000; i++ { wg.add(1) go increase() } wg.wait() log.println(cnt) } lock through a receive, unlock through a send func main() { cnt := 0 wg := sync.waitgroup{} lock := make(chan struct{}, 1) lock \u0026lt;- struct{}{} var increase = func() { defer wg.done() // lock through receive \u0026lt;-lock cnt++ // unlock lock \u0026lt;- struct{}{} } for i := 0; i \u0026lt; 10000; i++ { wg.add(1) go increase() } wg.wait() log.println(cnt) } use channels as counting semaphores buffered channels can be used as counting semaphores counting semaphores can be viewed as multi-owner locks. if the capacity of a channel is n, then it can be viewed as a lock which can have most n owners at any time.\ncounting semaphores are often used to enforce a maximum number of concurrent requests.\nthere are two manners to acquire one piece of ownership of a channel semaphore\nacquire ownership through a send, release through a receive acquire ownership through a receive, release through a send ","title":"Channel Use Case in Golang"},{"date":"2022-12-12","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/programming-language-wire-for-d/","summary":"Wire has two core concepts: providers and injectors.\nProviders The primary mechanism in Wire is the providers: a function that can produce a value.\ntype Foo struct { X int } func ProvideFoo() Foo { return Foo{X: 42} } Providers can also return errors","tags":["basic","golang","solid"],"text":"wire has two core concepts: providers and injectors.\nproviders the primary mechanism in wire is the providers: a function that can produce a value.\ntype foo struct { x int } func providefoo() foo { return foo{x: 42} } providers can also return errors\ninjectors an application wires up these providers with an injector: a function that calls providers in dependency order. with wire, you write the injector\u0026rsquo;s signature, then wire generates the function\u0026rsquo;s body.\n","title":"wire for D. D for Dependency Injection"},{"date":"2022-11-26","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/concurrency-pattern-pipeline-and-cancellation/","summary":"Go\u0026rsquo;s concurrency primitives make it easy to construct streaming data pipelines that make efficient use of I/O and multiple CPUs. This article introduces what is pipeline, how to to construct a pipeline and introduces techniques for dealing with failures cleanly.","tags":["concurrency pattern","golang"],"text":"go\u0026rsquo;s concurrency primitives make it easy to construct streaming data pipelines that make efficient use of i/o and multiple cpus. this article introduces what is pipeline, how to to construct a pipeline and introduces techniques for dealing with failures cleanly.\nwhat is a pipeline? a pipeline is a series of stages connected by channels, where each channel is a group of goroutines running the same function. in each stage, the goroutines\nreceive values from upstream via inbound channels perform some function on that data, usually producing new values send the values downstream via outbound channels each stage has any number of inbound and outbound channels, except the first and last stages, which have only outbound or inbound channels. the first stage is sometimes called the source and producer; the last stage, the sink or consumer.\nsquaring numbers consider a pipeline with three stages\nthe first stage, gen, is a function that converts a list of integers to a channel that emits the integers in the list.\n// the main function sets up the pipeline and runs the final stages // it receives values from the second stage and prints each one, until the channel is closed func main() { // a pipeline c := gen(2, 3) out := sq(c) fmt.println(\u0026lt;-out) fmt.println(\u0026lt;-out) } // the gen function starts a goroutine that sends the integers on the channel and closes the channel when all the values have been sent: func gen(nums ...int) \u0026lt;-chan int { out := make(chan int) go func() { for _, n := range nums { out \u0026lt;- n } close(out) }() return out } // the second stage, sq receives integers from a channel and returns a channel that emits the square of each received integer. // after the inbound channel is closed and this stage has sent all the values downstream, it closes the outbound channel func sq(in \u0026lt;-chan int) \u0026lt;-chan int { out := make(chan int) go func() { for n := range in { out \u0026lt;- n * n } close(out) }() return out } fan-in, fan-out stopping short there is a pattern to our pipeline functions:\nstages close their outbound channels when all the send operations are done. stages keep receiving value from inbound channels until those channel are closed. this pattern allows each receiving stage to be written as for range loop and ensures that all goroutines exit once all values have been successfully sent downstream.\nbut in real pipelines, stages don\u0026rsquo;t always receive all the inbound values. sometimes this is by design: the receiver may only need a subset of values to make progress. more often, a stage exit early because an inbound value represents an error in a earlier stages to stop producing values that later stage don\u0026rsquo;t need.\nfunc gen(nums ...int) (result\u0026lt;-chan int) { result = make(chan int) for _, num := range nums { // blocking operation result \u0026lt;- num } return } func main() { result := gen(2,3) fmt.println(result) // this is a resource leak: goroutines consume memory and runtime // resources, and heap references in goroutine stacks heap data from // being garbage collected. goroutines are not garbage collected; // they must exit on their own. fmt.println(runtime.numgoroutine()) // 2 } we need to arrange for the upstream stages of our pipeline to exit even when the downstream stages fail to receive all the inbound values. one way to do this is to change the outbound channels to have a buffer.\nc := make(chan int, 2) // buffer size 2 c \u0026lt;- 1 // succeeds immediately c \u0026lt;- 2 // succeeds immediately c \u0026lt;- 3 // blocks until another goroutine does \u0026lt;- c and receives 1 when the max number of values to be sent is known at channel creation time, a buffer can simplify the code.\nexplicit cancellation when main decides to exit without receiving all the values from out, it must tell the goroutines in the upstream stages to abandon the values they\u0026rsquo;re trying to send. it does so by sending values on channel called done.\nfunc gen(nums ...int) \u0026lt;-chan int { out := make(chan int, len(nums)) go func() { for _, n := range nums { out \u0026lt;- n } // close when all the values have been sent close(out) }() return out } func sq(in \u0026lt;-chan int) \u0026lt;-chan int { out := make(chan int) go func() { // consume inbound for v := range in { out \u0026lt;- v * v } close(out) }() return out } func merge(done \u0026lt;-chan struct{}, cs ...\u0026lt;-chan int) \u0026lt;-chan int { // we don\u0026#39;t know number of channel size out := make(chan int) var wg sync.waitgroup go func() { wg.wait() close(out) }() output := func(c \u0026lt;-chan int) { for n := range c { select { // consume data case out \u0026lt;- n: // stop send value case \u0026lt;-done: break } } wg.done() } wg.add(len(cs)) // consume input channel, exit sending goroutine of input channel for _, v := range cs { // create goroutine to consume input channel data go output(v) } return out } func main() { // create main goroutine g := gen(2, 3) // create 2 new goroutines then exit c1 := sq(g) // create a new goroutine and block c2 := sq(g) // create a new goroutine and block // 3 goroutines because no goroutine consume it done := make(chan struct{}) defer func() { close(done) // only main goroutine }() out := merge(done, c1, c2) fmt.println(\u0026lt;-out) } func printgoroutine() { time.sleep(time.second) fmt.println(\u0026#34;num goroutine\u0026#34;, runtime.numgoroutine()) } here are the guidelines for pipeline construction\nstages close their outbound channels when all the send operations are done. stages keep receiving value from inbound channels until those channels are closed or the senders are unblocked. digesting a tree references\nhttps://go.dev/blog/pipelines ","title":"Pipelines and cancellation"},{"date":"2022-11-26","image":"/images/reactive/rx.png","imageAlt":"","link":"https://idev-blog.web.app/posts/asynchronize-reactive-programming-in-golang/","summary":"ReactiveX, or Rx for short, is an API for programming with Observable streams. ReactiveX is a new, alternative way of asynchronous programming to callbacks, promises, and deferred. It is about processing streams of events of items, with events being any occurrences or changes within the system.","tags":["go","reactive"],"text":"reactivex, or rx for short, is an api for programming with observable streams. reactivex is a new, alternative way of asynchronous programming to callbacks, promises, and deferred. it is about processing streams of events of items, with events being any occurrences or changes within the system. a stream of events is called an observable.\nrxgo hot vs. cold observables in the rx world, there is a distinction between cold and hot observables. when the data is produced by the observable itself, it is a cold observable. when the data is produced outside the observable, it is a hot observable. usually, we don\u0026rsquo;t want to create a producer over and over again, we favor a hot observable.\n// create a hot observable using fromchannel operator ch := make(chan rxgo.item) go func() { for i := 0; i \u0026lt; 3; i++ { ch \u0026lt;- rxgo.of(i) } close(ch) }() observable := rxgo.fromchannel(ch) // first observer for item := range observable.observe() { fmt.println(\u0026#34;first observer\u0026#34;, item.v) } // second observer for item := range observable.observe() { fmt.println(\u0026#34;second observer\u0026#34;, item.v) } // result // // 0 // 1 // 2 // it means the first observer already consumed all items. // create a cold observable using defer operator: observable := rxgo.defer([]rxgo.producer{func(_ context.context, ch chan\u0026lt;- rxgo.item) { for i := 0; i \u0026lt; 3; i++ { ch \u0026lt;- rxgo.of(i) } }}) // in the case of a cold observable, the stream was created independent for // every observer for item := range observable.observe() { fmt.println(\u0026#34;first observable: \u0026#34;, item.v) } // in the case of a cold observable, the stream was created independent for // every observer for item := range observable.observe() { fmt.println(\u0026#34;second observable: \u0026#34;, item.v) } // result // 0 // 1 // 2 // 0 // 1 // 2 hot vs cold observable are not about how you consume items, it\u0026rsquo;s about where data is produced. good example for hot observable are price ticks from a trading exchange. and if you teach an observable to fetch products from a database, then yield them one by one, you will create the cold observable.\n","title":"Reactive Programming with Go"},{"date":"2022-11-26","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/tech-talk-reactive-programming-vertx/","summary":"","tags":["java","techtalk"],"text":"","title":"Reactive Programming, VertX"},{"date":"2022-11-25","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/net-work-cdn/","summary":"","tags":["network"],"text":"","title":"Content Delivery Network"},{"date":"2022-11-15","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/bigdata-hms/","summary":"We can simplify the Hive architecture to four components:\nThe runtime contains the logic of the query engine that translates the SQL - esque Hive Query Language (HQL) into MapReduce jobs that run over files stored in the filesystem.\nStorage component is simply that, it stores file in various formats and index structures to recall these files (JSON, CSV, ORC, Parquet, HDFS, Aws S3, GCS)","tags":["bigdata","trino"],"text":"we can simplify the hive architecture to four components:\nthe runtime contains the logic of the query engine that translates the sql - esque hive query language (hql) into mapreduce jobs that run over files stored in the filesystem.\nstorage component is simply that, it stores file in various formats and index structures to recall these files (json, csv, orc, parquet, hdfs, aws s3, gcs)\nin order for hive to process these files, it must have a mapping from sql tables in the runtime to files and directories in the storage component. to accomplish this, hive uses the hms (hive metastore service)\nhive connector the hive connector allows querying data stored in an apache hive data warehouse. hive is a combination of there components:\ndata files in varying formats (hdfs, s3) hms query language (hiveql) trino only uses the first two components: the data and the metadata. it does not use hiveql or any part of hive\u0026rsquo;s execution environment.\n","title":"Hive metastore service "},{"date":"2022-11-10","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/monitor-distributed-tracing-opentelemetry/","summary":"","tags":["tracing"],"text":"","title":"Distributed tracing, open telemetry"},{"date":"2022-11-08","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/linux-auto-connect-openvpn/","summary":"The OpenVPN is an open source Virtual Private Network (VPN) project. It creates secure connections over the Internet using a custom security protocol that utilizes SSL/TLS.\nIn this article, i will introduce naive solution for automatic connect to VPN using openvpn and systemd management.","tags":["linux"],"text":"the openvpn is an open source virtual private network (vpn) project. it creates secure connections over the internet using a custom security protocol that utilizes ssl/tls.\nin this article, i will introduce naive solution for automatic connect to vpn using openvpn and systemd management.\ncreate vpn script\n# create script connect to vpn mkdir -p ~/scripts cd ~/scripts touch vpn.sh sudo chmod +x vpn.sh install oauthtool\nsudo apt install oathtool ~/scripts/vpn.sh\nvpn_user=\u0026#34;\u0026#34; # insert vpn user here vpn_password=\u0026#34;\u0026#34; # insert vpn password here otp_key=\u0026#34;\u0026#34; # insert otp key here ovpn_file=\u0026#34;\u0026#34; # insert path to .ovpn file here. example /home/admicro-bigdata.ovpn vpn_auth=\u0026#34;$(oathtool -b --totp $otp_key)$vpn_password\u0026#34; echo $vpn_password | sudo -s bash -c \u0026#34;openvpn --config $ovpn_file --auth-user-pass \u0026lt;(echo -e \u0026#39;$vpn_user\\n$vpn_auth\u0026#39;) --daemon\u0026#34; create systemd service\ncd /lib/systemd/system sudo touch vpn.service /lib/systemd/system/vpn.service\n[unit] description=auto connect vccorp\u0026#39;s vpn service. [service] type=forking user=root # path to vpn file, example: /home/ngoctd/scripts/vpn.sh execstart=/bin/bash /home/ngoctd/scripts/vpn.sh # path to vpn file, example: /home/ngoctd/scripts/vpn.sh execreload= /bin/bash /home/ngoctd/scripts/vpn.sh execstop=sudo killall openvpn restart=on-failure restartsec=10s [install] wantedby=multi-user.target start,stop,auto restart service\nsudo systemctl daemon-reload # reload systemd daemon sudo systemctl enable vpn # auto start when vpn was killed sudo systemctl start vpn # start vpn sudo systemctl status vpn # check vpn status sudo systemctl stop vpn # stop vpn ","title":"Automatic connect to VPN"},{"date":"2022-11-08","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/programming-language-graceful-cancel-context-with-timeout-in-golang/","summary":"","tags":["basic","golang"],"text":"","title":"Cancel context with timeout in golang"},{"date":"2022-11-08","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/tech-talk-concurrent-is-not-parallelism/","summary":"","tags":[],"text":"","title":"Concurrency is not parallelism"},{"date":"2022-11-08","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/microservices-protocol-buffers/","summary":"","tags":["grpc","microservice"],"text":"","title":"Observer Pattern"},{"date":"2022-11-07","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/overview-grpc/","summary":"gRPC is a modern, open source remote procedure call (RPC) framework that can run anywhere. It enables client and server applications to communicate transparently, and makes it easier to build connected systems.\nIn gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object, making it easier for you to create disitributed applications can services.","tags":["grpc"],"text":"grpc is a modern, open source remote procedure call (rpc) framework that can run anywhere. it enables client and server applications to communicate transparently, and makes it easier to build connected systems.\nin grpc, a client application can directly call a method on a server application on a different machine as if it were a local object, making it easier for you to create disitributed applications can services. as in many rcp systems, grpc is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types. on the server side, the server implements this interface and runs a grpc server to handle client calls. on the client side, the client has a stub (referred to as just a client in some languages) that provides the same methods as the server.\nservice definition like many rpc systems, grpc is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types.\nservice helloservice { rpc sayhello(hellorequest) returns (helloresponse); } message hellorequest { string greeting = 1; } message helloresponse { string reply = 1; } four kinds of service method unary rpcs where the client sends a single request to the server and gets a single response back service unaryservice { rpc unary(requestr) returns (response); } server streaming rpcs where the client sends a request to the server and gets a stream to read a sequence of messages back. the client reads from the returned stream until there are no more messages. grpc guarantees message ordering within an individual rpc call. service serverstreaming { rpc streaming(request) returns (responses); } client streaming rpcs where the client writes a sequence of messages and sends them to the server, again using a provided stream. once the client has finished writing the messages, it waits for the server to read them and return its response. again grpc guaranteed message ordering within an individual rpc call. service clientstreaming { rpc streaming(requests) returns (response); } bidirectional streaming rpcs where both sides send a sequence of messages using a read-write stream. server could wait to receive all the client messages before writing its responses, or it could alternately read a message then write a message, or some other combination of reads and writes. the order of messages in each stream is preserved. service bidirectionalstreaming { rpc streaming(requests) returns (responses) } using the api grpc provides protocol buffer compiler plugins that generate client and server side code. grpc users typically call these apis on the client side and implement the corresponding api on the server side.\non the server side, the server implements the method declared by the service and runs a grpc server to handle client calls. the grpc infrastructure decodes incoming requests, executes service methods, and encodes service response. on the client side, the client has a local object known as stub that implements the same methods as the service. the client can then just call those methods on the local object, wrapping the parameters for the call in the appropriate protocol buffer message type. rpc life cycle what happen when a grpc client calls a grpc server method.\nunary rpc client sends a single request and gets back a single response.\nserver streaming rpc client streaming rpc bidirectional streaming rpc deadlines/timeouts grpc allows clients to specify how long they are willing to wait for an rpc to complete before the rpc is terminated with a deadline_exceeded error.\nrpc termination in grpc, both the client and server independent and local determinations of the success of the call, and their conclusions may not match. this means that, for example, you could have an rpc that finishes successfully on the server side (\u0026ldquo;i have sent all my responses!\u0026rdquo;) but fails on the client side (\u0026ldquo;the responses arrived after my deadline!\u0026rdquo;). it\u0026rsquo;s also possible for a server to decide to complete before a client has sent all its requests.\ncancelling an rpc either the client or the server can cancel an rpc at any time. a cancellation terminations the rpc immediately so that no further work is done.\nchannels a grpc channel provides a connection to a grpc server on a specified host and port. a channel has state, including connected and idle.\n","title":"GRPC Overview"},{"date":"2022-11-07","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/overview-protocol-buffer/","summary":"Protocol buffers provide a language-neutral, platform-neutral, extensible mechanism for serializing structured data in a forward-compatible and backward-compatible way. It\u0026rsquo;s like JSON, except it\u0026rsquo;s smaller and faster, and it generates native language bindings.\nProtocol buffers are a combination of the definition language (created in .","tags":["grpc"],"text":"protocol buffers provide a language-neutral, platform-neutral, extensible mechanism for serializing structured data in a forward-compatible and backward-compatible way. it\u0026rsquo;s like json, except it\u0026rsquo;s smaller and faster, and it generates native language bindings.\nprotocol buffers are a combination of the definition language (created in .proto files), the code that the proto compiler generates to interface with data, language-specific runtime libraries, and the serialization format for data that is written to a file (or sent across a network connection).\nsome of the advantages of using protocol buffers\ncompact data storage fast parsing availability in many programming languages optimized functionality through automatically-generated classes when are protocol buffers not a good fit?\n","title":"Protocol Buffers Overview"},{"date":"2022-11-05","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/programming-language-channel-in-golang/","summary":"Channel is an important built-in feature in Go. It is one of the features makes Go unique. Channel makes concurrent programming convenient, fun and lowers the difficulties of concurrent programming. Channel mainly acts as a concurrency synchronization technique. To understand channels better, the internal structure of channels and some implementation details by the standard Go compiler/runtime are also simply described.","tags":["basic","golang"],"text":"channel is an important built-in feature in go. it is one of the features makes go unique. channel makes concurrent programming convenient, fun and lowers the difficulties of concurrent programming. channel mainly acts as a concurrency synchronization technique. to understand channels better, the internal structure of channels and some implementation details by the standard go compiler/runtime are also simply described.\nchannel introduction don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\ncommunicating by sharing memory and sharing by communicating are two programming manners in concurrent programming. when goroutines communicate by sharing memory, we use traditional concurrency sychronization techniques, such as mutex locks, to protect the shared memory to prevent data racts.\ngo also provides another concurrency sychronization technique, channel. channels make goroutines share memory by communicating. we can view a channel as an internal fifo queu within a program. some goroutines send values to the queue (the channel) and some other goroutines receive values from the queue.\nalong with transfering values (through channels), the ownership of some values may also be transferred between goroutines (ownership on logic view). when a goroutine send a value to a channel, we can view the goroutine releases the ownership of some values. when a goroutine receives a value from a channel, we can view the goroutine acquires the ownerships of some values.\nchannel value comparisons all channel types are comparable types. if one channel value is assigned to another, the two channels share the same underlying part(s). in other words, those two channels represent the same internal channel object. the result of comparing them is true.\ndetailed explanations for channel operations operation a nil channel a closed channel a not-closed non-nil channel close panic panic success send value to block for ever panic block or succeed to send receive value from block for ever never block block or success to receive to bettern understand channel types and values, and to make some explainations easier, looking in the raw internal structures of internal channel objects is very helpful.\nwe can think of each channel consistin of three queues internally:\nthe receiving goroutine queue (generally fifo). the queue is a linked list without size limitation. goroutines in this queue are all in blocking state and waiting to receive values from that channel.\nthe sending goroutine queue (generally fifo). the queue is also a linked list without size limitation. goroutines in this queue are all in blocking state and waiting to send values to that channel.\nthe value buffer queue (absolutely fifo). this is a circular queue. its size is equal to the capacity of the channel. if the current number of values stored in the value buffer queue of the channel reaches the capacity of the channel, the channel is called in full status. if no values are store in the value buffer queue of the channel currently, the channel is called in empty status. for a zero-capacity (unbuffered) channel is also in both full and empty status.\neach channel internally holds a mutex lock which is used to avoid data races in all kinds of operations\nchannel operation: try to receive when a goroutine r tries to receive a value from a not-closed non-nil channel, the goroutine r will acquire the lock associated with the channel firstly, the do the following steps until one condition is satisfied.\ncheck buffer, if the value buffer queue of the channel is not empty. the receiving goroutine queue of the channel must be empty ( buffer != empty =\u0026gt; receiveing queue == emtpy ). the goroutine r will receive (by unshifting) a value from the value buffer queue. if the sending goroutine queue of the channel is also not empty, a sending goroutine will be unshifted out of the sending goroutine queue and resumed to running state again. the value the just unshifted sending goroutine trying to send will be pushed into the value buffer queue of the channel. the receiving goroutine r continues running. for this scenario, the channel receive operation is called a non-blocking operation. the goroutine r will receive a value from the value buffer queue. the goroutine r will recceive a value from the value buffer queue. sending goroutine is not empty. goroutine s send value to buffer and enter running state again. -\u0026gt; the receiving goroutine r continues running. the channel receive operation is called a non-blocking operation\ncheck buffer, the value buffer of the channel is empty. if the sending goroutine queue of the channel is not empty, in which case the channel must be an unbuffered channel, the receiving goroutine r will unshift value from a send goroutine. the just unshifted sending goroutine will get unblocked and resumed to running state again. -\u0026gt; the receiving goroutine r continues running. the channel receive operation is called a non-blocking operation\nif value buffer queue and the sending goroutine queue of the channel are both emtpy, the goroutine r will be pushed into the receiving goroutine queue of the channel and enter (and stay in) blocking state. it may be resumed to running state when another goroutine sends a value to the channel later. -\u0026gt; the receiving goroutine r enter blocking state. the channel receive operation is called a blocking operation\nchannel operation: try to send when a goroutine s tries to send a value to a not-closed non-nil channel, the goroutine s will acquire the lock associated with the channel firstly, then do the following steps until one step condition is satisfied.\ncheck receiving goroutine queue. if the receiving goroutine queue of the channel is not empty, in which case the value buffer queue of the channel must be empty, the sending goroutine s will unshift a receiving goroutine from the receiving goroutine queue of the channel and send the value to the just unshifted receiving goroutine. the just unshifted receiving goroutine will get unblocked and resumed to running state again. -\u0026gt; the sending goroutine s continues running. the channel send operation is called a non-blocking operation\ncheck receiving goroutine queue (empty), check buffer queue ( not full ), in which case the sending goroutine queue must be also empty, the value the sending goroutine s trying to send will be pushed into the value buffer queue. -\u0026gt; the sending goroutine s continues running. the channel send operation is called a non-blocking operation\ncheck receiving goroutine queue (empty), check buffer queue ( full ), the sending goroutine s will be pushed into the sending goroutine queue of the channel and enter (and stay in) blocking state. it may be resumed to running state when another goroutine receives a value from the channel later. -\u0026gt; the sending goroutine s enter blocking. the channel send operation is called a blocking operation\nonce a non-nil channel is closed, sending a value to the channel will produce a runtime panic in the current goroutine. note sending data to a closed channel is viewed as a non-blocking operation.\nchannel operation: try to close when a goroutine tries to close a not-closed non-nil channel, once the goroutine has acquired the lock of the channel, both of the following two steps will be performed by the following order.\nif the receiving goroutine queue of the channel is not empty, in which case the value buffer of the channel must be empty, all the goroutines in the receiving goroutine queue of the channel will be unshifted one by one, each of themm will receive a zero value of the elemenet type of the channel and be resumed to running state. if the sending goroutine queue of the channel is not empty, all the goroutines in the sending goroutine queue of the channel will be unshifted one by one and each of them will produce a panic for sending on a closed channel. this is the reason why we should avoid concurrent send and close operations on the same channel. after a channel is closed, the values which have been already pushed into the value buffer of the channel are still there.\nafter a non-nil channel is closed, channel receive operations os the channel will never block\nsome facts about the internal queues of a channel if the channel is closed, both its sending and receiving goroutine queue must be empty, but its value buffer may not be empty. at any time, if the value buffer is not empty, then its receiving goroutine queue must be empty. at any time, if the value buffer is not full, then its sending goroutine queue must be empty. if the channel is buffered, then at time, at least one of the channel\u0026rsquo;s goroutine queues must be empty (sending, receiving or both). if the channel is unbuffered, most of the time one of its sending goroutine queue and the receiving goroutine queue must be empty, with one exception. the exception is that a goroutine may be pushed into both of the two queues when execution a select control flow code block. references channel use case go101\n","title":"Channel in Golang"},{"date":"2022-11-05","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/concurrency-pattern-fan-in-fan-out/","summary":"Fan-in Fan-out is a way of Multiplexing and De-Multiplexing in golang. Fan-in refers to processing multiple input data and combining into a single entity. Fan-out is the exact opposite, dividing the data into multiple smaller chunks,\nYou\u0026rsquo;ve got a pipeline set up.","tags":["concurrency pattern","golang"],"text":"fan-in fan-out is a way of multiplexing and de-multiplexing in golang. fan-in refers to processing multiple input data and combining into a single entity. fan-out is the exact opposite, dividing the data into multiple smaller chunks,\nyou\u0026rsquo;ve got a pipeline set up. data is flowing through your system beautifully. sometimes, stages in your pipeline can be computationally expensive. when this happens, upstream stages in your pipeline can become blocked while waiting for your expensive stages to complete.\none of the interesting properties of pipelines is the ability they give you to operate on the stream of data using a combination of separate, often reordered stages. maybe that would help improve the performance of the pipeline. in fact, it turns out it can, and this pattern has a name: fan-out, fan-in.\nfan in,fan out fan-out is a term to describe the process of starting multiple goroutines to handle input from the pipeline.\nyou might consider fanning out one of your stages if both of the following apply:\nit doesn\u0026rsquo;t rely on values that the state had calculated before. it takes a long time to run. (system call, a heavy cpu job, \u0026hellip;) the property of order-independence is important because you have no guarantee in what order concurrent copies of your stage will run, nor in what order they will return.\nmultiple functions can be read from the same channel until that channel is closed; this is called fan-out. this provides a way to distribute work amongst a group of workers to parallelize cpu use and i/o.\na function can read from multiple inputs and proceed until all are closed by multiplexing then input channels onto a single channel that\u0026rsquo;s closed when all the inputs are closed. this is called fan-in.\n// the first stage, gen is a function that converts a list of integers to a channel // that emits the integers in the list. the gen function starts a goroutine that sends // the integers on the channel and closes the channel when all the values have been sent func gen(nums ...int) \u0026lt;-chan int { out := make(chan int) go func() { for _, n := range nums { out \u0026lt;- n } // close when all the values have been sent close(out) }() return out } // the second stage, sq receives integers from a channel and returns a channel that emits the square // of each received integer. after the inbound channel is closed and this stage has sent all the values // downstream, it closes the outbound channel func sq(in \u0026lt;-chan int) \u0026lt;-chan int { out := make(chan int) go func() { // consume inbound for v := range in { out \u0026lt;- v * v } close(out) }() return out } func merge(cs ...\u0026lt;-chan int) \u0026lt;-chan int { out := make(chan int) var wg sync.waitgroup output := func(c \u0026lt;-chan int) { for n := range c { out \u0026lt;- n } wg.done() } wg.add(len(cs)) for _, v := range cs { go output(v) } go func() { wg.wait() close(out) }() return out } func main() { in := gen(2, 3) // distribute the sq work across two goroutines that both read from in // fan-out c1 := sq(in) c2 := sq(in) // consume the merged output from c1 and c2 for n := range merge(c1, c2) { fmt.println(n) } } ","title":"Fan out, Fan in"},{"date":"2022-11-05","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/concurrency-pattern-generator-future/","summary":"Generator Pattern allows the consumer of the data produced by the generator to run in parallel when the generator function is busy computing the next value.\nA Future indicates any data that is needed in future but its computation can be started in parallel so that it can be fetched from the background when needed.","tags":["concurrency pattern","golang"],"text":"generator pattern allows the consumer of the data produced by the generator to run in parallel when the generator function is busy computing the next value.\na future indicates any data that is needed in future but its computation can be started in parallel so that it can be fetched from the background when needed.\ngenerator generator pattern is used to generator a sequence of values which is used to produce some output. this allows the consumer of the data produced by the generator to run in parallel when the generator function is busy computing the text value.\nfunc fib(n int) chan int { c := make(chan int) go func() { // next state is depend on previous state for i, j := 0, 1; i \u0026lt; n; i, j = i+j, i { c \u0026lt;- i } close(c) }() return c } func consumer(c chan int) { for v := range c { fmt.println(v) } } the generator and the consumer can work concurrently (maybe in parallel) as the logic involved in both are different.\nfuture a future indicates any data that is needed in future but its computation can be started in parallel so that it can be fetched from the background when needed. mostly, futures are used to send asynchronous http request.\ntype data struct { body []byte error error } func futuredate(url string) \u0026lt;- chan data { c := make(chan data, 1) go func(){ resp, err := http.get(url) if err != nil { c \u0026lt;- data{ body: nil, error: err, } return } body, err := ioutil.readall(resp.body) resp.body.close() if err != nil { c \u0026lt;- data{ body: nil, error: err, } return } c \u0026lt;- data{ body: body, error: err, } }() return nil } the actual http request is done asynchronously in a goroutine. the main function can continue doing other things. when the result is needed, we read the result from the channel. if the request isn\u0026rsquo;t finished yet, the channel will block until the result is ready.\ndifferent between generator and future in generator pattern, we generate next state base on previous state (maybe not), but it purpose is compute many things in background. in future pattern, we use goroutine to execute an \u0026ldquo;heavy job\u0026rdquo; (only one job).\n","title":"Generator and Future Pattern"},{"date":"2022-11-05","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/programming-language-go-memory-order/","summary":"","tags":["basic","golang"],"text":"","title":"Golang Memory Order"},{"date":"2022-11-04","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/design-pattern-decorator/","summary":"I used to think real men subclassed everything. That was until I learned the power of extension at runtime, rather than at compile time.\nJust call this chapter \u0026ldquo;Design Eye for Inheritance Guy.\u0026rdquo; We\u0026rsquo;ll re-examine the typical overuse of inheritance and you\u0026rsquo;ll learn how to decorate your classes at runtime using a form of object composition.","tags":["design pattern"],"text":"i used to think real men subclassed everything. that was until i learned the power of extension at runtime, rather than at compile time.\njust call this chapter \u0026ldquo;design eye for inheritance guy.\u0026rdquo; we\u0026rsquo;ll re-examine the typical overuse of inheritance and you\u0026rsquo;ll learn how to decorate your classes at runtime using a form of object composition. why? once you know the techniques of decorating, you\u0026rsquo;ll be able to give your objects new responsibilities without making any code changes to the underlying classes.\nwelcome to starbuzz coffee starbuzz coffee has made a name for itself as the fastest-growing coffee shop. when they first went into business they designed their classes like this\u0026hellip;\nin addition to your coffee, you can also ask for several condiments like steamed milk, soy, and mocha\u0026hellip;\nit\u0026rsquo;s pretty obvious that starbuzz has created a maintenance nightmare for themselves. what happens when the price of milk goes up? what do they do when they add a new caramel topping.\nsolution: keep track of the condiments\npublic class beverage { public string description; public double milk; public double soy; public double mocha; public double cost() { // check condiments here } } public class darkroast extends beverage { public darkroast() { description = \u0026#34;dark road description\u0026#34;; } public double cost() { } } a: see, five classes total. this is definitely the way to go. b: i can see more potential problems with this approach by thinking about how th design might need to change in the future.\nprice changes for condiments will force us to alter existing code. new condiments will force us to add new methods and alter the cost method in the superclass we may have new beverages. for some of these beverages (tea), the condiments may not be appropriate, yet the tea subclass will still inherit methods like haswhip(). what if a customer wants a double mocha? constructing a drink order with decorators decorators have the same supertype as the objects they decorate. you can use one or more decorators to wrap an object give that the decorator has the same supertype as the object it decorates, we can pass around a decorated object in place of the original wrapped object the decorator adds its own behavior before and/or after delegating to the object it decorates to do the rest of the job. objects can be decorated at any time, so we can decorate objects dynamically at runtime as many decorators as we like. references\nhead first design pattern summary thinking beyond the maintenance problem, which of the design principles.\nwhen i inherit behavior by subclassing, that behavior is set statically at compile time. in addition, all subclasses must inherit the same behavior. however, i can extend an object\u0026rsquo;s behavior through composition, then i can do this dynamically at runtime.\nthe decorator pattern attaches additional responsibilities to an object dynamically. decorators provide a flexible alternatively to subclassing for extending functionality.\n129\n","title":"Decorator pattern"},{"date":"2022-11-04","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/overview-trino/","summary":"Trino is a distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources.\nSince Trino is being called a database by many members of the community, it makes sense to begin with a definition of what Trino is not.","tags":["trino"],"text":"trino is a distributed sql query engine designed to query large data sets distributed over one or more heterogeneous data sources.\nsince trino is being called a database by many members of the community, it makes sense to begin with a definition of what trino is not. trino is not a general-purpose relation database. it is not a replacement for databases like mysql, postgresql or oracle.\nwhat trino is trino is a tool designed to efficiently query vast amount of data using distributed queries. if you work with terabytes or petabytes of data, you are likely using tools that interact with hadoop and hdfs.\ntrino was designed as an alternative to tools that query hdfs using pipelines of mapreduce jobs, such as hive or pig, but trino it not limited to hdfs. trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as cassandra.\ntrino was designed to handle data warehousing and analytics: data analysis, aggregating large amount of data and producing reports. these workloads are often classified as olap.\ntrino component: coordinator ( server ) the trino coordinator is the server that is responsible for parsing statements, planning queries, and managing trino worker nodes. every trino installation must have at least one trino coordinator alongside one or more trino workers. coordinators communicate with workers and clients using rest api.\ntrino component: worker ( server ) the trino worker is responsible for executing tasks and processing data. worker nodes fetch data from connectors and exchange intermediate data with each other. the coordinator is responsible for fetching results from the workers and returning the final results to the client.\nwhen a trino worker process starts up, it advertises itself to the discovery server in the coordinator, which makes it available to the trino coordinator for task execution.\nworkers communicate with other workers and trino coordinators using a rest api.\ntrino component: connector ( data source ) a connector adapts trino to a data source such as hive or a relational database. you can think of a connector the same way you think of a driver for a database. connector implements trino\u0026rsquo;s spi.\ntrino component: catalog ( data source ) trino component: schema ( data source ) hive metadata storage tl;dr: the hive connector is what you use in trino for reading data from object storage that is organized according to the rules laid out by hive, without using the hive runtime code.\nreferences\ntrino documentation ","title":"Trino Overview"},{"date":"2022-11-03","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/design-pattern-observer/","summary":"Keeping your Objects in the Know, you don\u0026rsquo;t want to miss out when something interesting happens.\nThe Observer Pattern defines a one-to-many dependency between objects so that when one object changes state, all of its dependents are notified and updated automatically.","tags":["design pattern"],"text":"keeping your objects in the know, you don\u0026rsquo;t want to miss out when something interesting happens.\nthe observer pattern defines a one-to-many dependency between objects so that when one object changes state, all of its dependents are notified and updated automatically.\npublishers + subscribers = observer pattern ","title":"Observer Pattern"},{"date":"2022-10-13","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/poem-hai-%C4%91%E1%BB%A9a-tr%E1%BA%BB/","summary":"Hai đứa trẻ lớn lên nơi rừng núi\nXung quanh nhà nào là cỏ, là cây\nHai đứa trẻ là chị là em, và là bạn\nBạn bè xung quanh, hiếm lắm ấy mà.\nHai đứa trẻ nhiều đêm khó ngủ","tags":["thơ"],"text":"hai đứa trẻ lớn lên nơi rừng núi\nxung quanh nhà nào là cỏ, là cây\nhai đứa trẻ là chị là em, và là bạn\nbạn bè xung quanh, hiếm lắm ấy mà.\nhai đứa trẻ nhiều đêm khó ngủ\nbố mẹ đi rẫy mãi chưa về\nhai đứa trẻ ngồi bên ngọn nến sáng\nchỉ bóng trên tường, gọi mẹ xưng con.\nhai đứa trẻ rời xa nơi nương náu\nbố mẹ lo chuyện học hành mai sau\nhai đứa trẻ là chị là em, không là bạn\nnhảy dây, bị mắt, thú vị hơn nhiều.\nhai đứa trẻ mai này rồi cũng lớn\nđứa rời xóm làng, đứa tìm thủ đô\nhai đứa trẻ hiếm nào khi hỏi chuyện\nchuyện học hành, chuyện tương lai, chuyện mai sau\nhai đứa trẻ, hai đứa trẻ ấy không còn\u0026hellip;\n","title":"Hai Đứa Trẻ"},{"date":"2022-10-02","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/programming-language-golang-mistake/","summary":"All about golang at basic level in one post. Content in this post i have learned in go101 book and on the internet. At this time, i think it can be misconcept in golang.\nBasic Types and Basic Value Literals rune type (a.","tags":["basic","golang"],"text":"all about golang at basic level in one post. content in this post i have learned in go101 book and on the internet. at this time, i think it can be misconcept in golang.\nbasic types and basic value literals rune type (a.k.a int32 type), are special integer types. a rune value is intended to store a unicode point. a rune literal is expressed as one or more characters enclosed in a pair of quotes, for example \u0026lsquo;a\u0026rsquo; (the unicode value of character a is 97). we should know that some unicode characters are composed of more than one code points, for example \u0026lsquo;a\u0026rsquo;, \u0026lsquo;\\x61\u0026rsquo;, \u0026lsquo;\\141\u0026rsquo; are the same (the unicode value is 97).\nfunc main() { println(\u0026#39;a\u0026#39; == 97) println(\u0026#39;a\u0026#39; == \u0026#39;\\x61\u0026#39;) println(\u0026#39;a\u0026#39; == \u0026#39;\\141\u0026#39;) println(\u0026#39;a\u0026#39; == \u0026#39;\\u0061\u0026#39;) } ","title":"Golang All in One"},{"date":"2022-03-11","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/poem-m%C6%B0a/","summary":"Mưa,\nMưa trên mái hiên nhà\nĐến tận cánh đồng xa\nMưa,\nMưa hoài, mưa xối xả\nKìa ai đang thả thuyền lá\nKí ức trôi\nTrôi mãi tận miền xa.","tags":["thơ"],"text":"mưa,\nmưa trên mái hiên nhà\nđến tận cánh đồng xa\nmưa,\nmưa hoài, mưa xối xả\nkìa ai đang thả thuyền lá\nkí ức trôi\ntrôi mãi tận miền xa.\n","title":"Mưa"},{"date":"2021-11-20","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/poem-l%E1%BB%ADa-h%E1%BB%93ng/","summary":"Có một đốm than nhỏ\nThắp lên ngọn lửa hồng\nDành trọn nhiệt huyết nồng\nTháp sáng ước mơ to.","tags":["thơ"],"text":"có một đốm than nhỏ\nthắp lên ngọn lửa hồng\ndành trọn nhiệt huyết nồng\ntháp sáng ước mơ to.\n","title":"Lửa Hồng"},{"date":"0001-01-01","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/authentication-jwt/","summary":"Authentication Json Web Token In its compact form, JSON Web Tokens consist of three parts separated by dots (.) which are:\nHeader Payload Signature Header The header typically consists of two parts: the type of the token, which is JWT, and the signing algorithm being used, such as HMAC SHA256 or RSA","tags":[],"text":"authentication json web token in its compact form, json web tokens consist of three parts separated by dots (.) which are:\nheader payload signature header the header typically consists of two parts: the type of the token, which is jwt, and the signing algorithm being used, such as hmac sha256 or rsa\n{ \u0026#34;alg\u0026#34;: \u0026#34;hs256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;jwt\u0026#34; } then, this json is base64url encoded to form the first part of the jwt.\npayload\nthe second part of the token is the payload, which contains the claims. claims are statements about an entity (typically, the user) and additional data. there are three types of claims: registered, public, and private claims.\nregistered claims: there are a set of predefined claims which are not mandatory but recommended, to provide a set of useful iss(issuer), exp(expiration time), sub(subject), aud(audience) public claims: private claims: these are the custom claims created to share information between parties that agree on using them and are neither registered or public claims. { \u0026#34;sub\u0026#34;: \u0026#34;123456789\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;john\u0026#34;, \u0026#34;admin\u0026#34;: true } the payload is then base64url encoded to form the second part of the json web token.\nsignature\nto create the signature part you have to take the encoded header, the encoded payload, a secret, the algorithm specified in the header, and sign that. the signature is used to verify the message wasn\u0026rsquo;t changed along the way, and, in the case of tokens signed with a private key, it can also verify that the sender of the jwt is who it says it is.\nhow do json web tokens work?\nin authentication, when the user successfully logs in using their credentials, a json web token will be returned. since tokens are credentials,, greate care must be taken to prevent security issues. in general, you should not keep tokens longer than required.\nyou also should not store sensitive session data in browser storage due to lack of security.\nwhenever the user wants to access a protected route or resource, the user agent should send the jwt, typically in the authorization header using the bearer schema.\nthe content os the header should look like the following:\nauthorization: bearer \u0026lt;token\u0026gt; ","title":""},{"date":"0001-01-01","image":"","imageAlt":"","link":"https://idev-blog.web.app/posts/protobuf-backward-and-forward-compatibility/","summary":"What exactly are protocol buffers? Protocol Buffers provide a language-neutral, platform-neutral, extensible mechanism for serializing structured data in a forward-compatible and backward-compatible way. It\u0026rsquo;s like JSON, except it\u0026rsquo;s smaller and faster, and it generates native language bindings.\nSome of the advantages of using protocol buffers include: Compact data storage Fast parsing Availability in many programming languages Optimized functionality through automatically-generated classes Understanding Backward and Forward Compatibility There are at least two parts to any protobuf system, the sender and the receiver.","tags":[],"text":"what exactly are protocol buffers? protocol buffers provide a language-neutral, platform-neutral, extensible mechanism for serializing structured data in a forward-compatible and backward-compatible way. it\u0026rsquo;s like json, except it\u0026rsquo;s smaller and faster, and it generates native language bindings.\nsome of the advantages of using protocol buffers include: compact data storage fast parsing availability in many programming languages optimized functionality through automatically-generated classes understanding backward and forward compatibility there are at least two parts to any protobuf system, the sender and the receiver. if either one can be upgraded to a new message format, and the system functionality continues uninterrupted then the message protocol is both forward and backward compatible.\nbackward compatibility\nif a client that was updated to a new message type but it still able to understand the previous message type then the message change is backward compatible. backward compatibility is being able to understand messages from a previous version.\nforward compatibility\nif a message if changed and a non-updated client can still understand and process the message then the message change if forward compatible. forward compatibility is being able to understand messages from a future version.\nwith protocol buffers, it a sender is upgraded, the receiver can still understand messages if it is forward compatible.\n","title":""}]
}

